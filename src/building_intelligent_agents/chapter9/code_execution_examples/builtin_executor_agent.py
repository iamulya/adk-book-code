from google.adk.agents import Agent
from google.adk.code_executors import BuiltInCodeExecutor # Key import
from google.adk.runners import InMemoryRunner
from google.genai.types import Content, Part
from building_intelligent_agents.utils import load_environment_variables, create_session, DEFAULT_LLM
load_environment_variables()

# This agent will use the model's internal code interpreter.
# Ensure you are using a model that supports this (e.g., Gemini 1.5 Pro or later).
# For Gemini API, this is often enabled by default.
# For Vertex AI, you might need to ensure the model version supports it.

code_savvy_agent_builtin = Agent(
    name="code_savvy_agent_builtin",
    model=DEFAULT_LLM, # A model with built-in code execution
    instruction="You are a helpful assistant that can write and execute Python code to answer questions, especially for calculations or data analysis. When you write code, it will be automatically executed.",
    code_executor=BuiltInCodeExecutor() # Assign the executor
)

if __name__ == "__main__":
    runner = InMemoryRunner(agent=code_savvy_agent_builtin, app_name="BuiltInCodeApp")
    session_id = "s_builtin_code_test"
    user_id = "builtin_user"
    create_session(runner, session_id, user_id)
    prompts = [
        "What is the factorial of 7?",
        "Calculate the square root of 12345.",
        "Generate a list of the first 10 prime numbers."
    ]

    async def main():
        for prompt_text in prompts:
            print(f"\\nYOU: {prompt_text}")
            user_message = Content(parts=[Part(text=prompt_text)], role="user")
            print("ASSISTANT: ", end="", flush=True)
            async for event in runner.run_async(user_id=user_id, session_id=session_id, new_message=user_message):
                # The trace for this would show the model generating code,
                # and then a `code_execution_result` part directly from the model,
                # followed by the model's textual interpretation.
                if event.content and event.content.parts:
                    for part in event.content.parts:
                        if part.text:
                            print(part.text, end="", flush=True)
                        elif part.executable_code: # Code generated by LLM
                            print(f"\\n  CODE BLOCK:\\n{part.executable_code.code.strip()}\\n  END CODE BLOCK", end="")
                        elif part.code_execution_result: # Result from model's interpreter
                            print(f"\\n  EXECUTION RESULT: {part.code_execution_result.outcome}\\n  OUTPUT:\\n{part.code_execution_result.output.strip()}\\n  END EXECUTION RESULT", end="")
            print()

    import asyncio
    asyncio.run(main())